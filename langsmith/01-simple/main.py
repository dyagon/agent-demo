import os
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å¼€å¯ LangSmith è‡ªåŠ¨è¿½è¸ªåŠŸèƒ½
os.environ["LANGCHAIN_PROJECT"] = "LangSmith-Tongyi-Project" 
os.environ["LANGCHAIN_TRACING_V2"] = "true"

llm = ChatTongyi(
    model_name="qwen-turbo", 
    temperature=0.7
)

# åˆ›å»º Prompt æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­å›½ä¼ ç»Ÿæ–‡åŒ–çš„ä¸“å®¶ã€‚è¯·ç”¨ä¼˜ç¾ã€è¯—æ„çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
    ("user", "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ï¼š{concept}")
])

# åˆ›å»ºè¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# æ‹¼æ¥æˆ Chain
chain = prompt | llm | parser

# ==========================================
# 3. è¿è¡Œåº”ç”¨
# ==========================================
print("æ­£åœ¨è°ƒç”¨é€šä¹‰åƒé—®ï¼Œè¯·ç¨å€™...\n")
response = chain.invoke({"concept": "äºŒåå››èŠ‚æ°”ä¸­çš„'æƒŠè›°'"})

print("ğŸ¤– æ¨¡å‹å›å¤ï¼š")
print(response)
