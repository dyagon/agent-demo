"""
å¤šå‘é‡æ£€ç´¢ + å¯¹è¯å¼ QAï¼šStreamlit + LangGraphï¼ŒMultiVectorRetriever è¿”å›çˆ¶æ–‡æ¡£ã€‚
"""
import streamlit as st
from graph import build_qa_graph, QAState


@st.cache_resource
def get_graph():
    return build_qa_graph()


def main():
    st.set_page_config(page_title="æ–‡æ¡£ QAï¼ˆå¤šå‘é‡ï¼‰", page_icon="ğŸ“š")
    st.title("ğŸ“š æ–‡æ¡£ QAï¼ˆå¤šå‘é‡æ£€ç´¢ï¼‰")
    st.caption("æ‘˜è¦å‘é‡æ£€ç´¢ â†’ è¿”å›çˆ¶æ–‡æ¡£ â†’ å¯¹è¯å¼é—®ç­”ï¼ˆMultiVectorRetriever + LangGraphï¼‰")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for q, a in st.session_state.chat_history:
        with st.chat_message("user"):
            st.markdown(q)
        with st.chat_message("assistant"):
            st.markdown(a)

    if prompt := st.chat_input("è¾“å…¥é—®é¢˜..."):
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”â€¦"):
                graph = get_graph()
                result = graph.invoke(
                    QAState(
                        question=prompt,
                        chat_history=st.session_state.chat_history,
                        retrieved_docs=[],
                        answer="",
                    )
                )
            st.markdown(result["answer"])
        st.session_state.chat_history.append((prompt, result["answer"]))
        st.rerun()


if __name__ == "__main__":
    main()
